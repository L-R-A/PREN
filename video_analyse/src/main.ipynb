{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preconditions\n",
    "\n",
    "import os\n",
    "import json\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import keras.api._v2.keras as keras\n",
    "from keras.applications import VGG16\n",
    "\n",
    "import random\n",
    "import helper as hp\n",
    "\n",
    "MODEL_NAME = \"v5_data_10000\" + \".keras\"\n",
    "MODEL_PATH = os.path.join(\"..\", \"tmp\", \"models\")\n",
    "\n",
    "RESSOURCES_PATH = os.path.join(\"..\", \"tmp\", \"train\", \"ressources\")\n",
    "TRAIN_DATA_FOLDER = \"ea0205e6-c5c6-4b1d-afb6-6f0e36e7acee\"\n",
    "\n",
    "# Image properties\n",
    "IMAGE_HEIGHT_PX = 120\n",
    "IMAGE_WIDTH_PX = 160\n",
    "CROP_HEIGHT_PX = 5\n",
    "CROP_WIDTH_PX = 20\n",
    "\n",
    "NUM_CHANNELS = 3\n",
    "NUM_CLASSES = 4\n",
    "NUM_POSITIONS = 8\n",
    "\n",
    "color_mapping = { 'red': 0, 'yellow': 1, 'blue': 2, '': 3 }\n",
    "label_mapping = { 0: 'red', 1: 'yellow', 2: 'blue', 3: ''}\n",
    "\n",
    "NORMALIZE_VALUE = 255\n",
    "\n",
    "def map_labels_to_nummeric(label):\n",
    "    mapped_label = []\n",
    "\n",
    "    for pos in label.values():\n",
    "        mapped_label.append(color_mapping[pos])\n",
    "\n",
    "    return mapped_label\n",
    "\n",
    "# Normalize the images so that all values are between 0 and 1\n",
    "def normalize_images(images):\n",
    "    return images / NORMALIZE_VALUE\n",
    "\n",
    "\n",
    "# Data loading\n",
    "IN_DEBUG_MODE = False\n",
    "IMAGE_FOLDER = \"Images\"\n",
    "LABELS_FOLDER = \"Labels\"\n",
    "JSON_NAME = \"scene_results.json\"\n",
    "\n",
    "def get_data(stage):\n",
    "    labels = []\n",
    "    images = []\n",
    "\n",
    "    scene_results_path = os.path.join(RESSOURCES_PATH, TRAIN_DATA_FOLDER, stage, JSON_NAME)\n",
    "\n",
    "    if IN_DEBUG_MODE:\n",
    "        print(\"CURRENT STAGE: \" + stage + \"\\n\")\n",
    "        print(\"READING SCENE RESULTS AT: \" + scene_results_path)\n",
    "\n",
    "    with open(scene_results_path, 'r') as file:\n",
    "        scene_results = json.load(file)\n",
    "\n",
    "    for result in scene_results:\n",
    "        img = [];\n",
    "\n",
    "        image_not_found = False\n",
    "\n",
    "        if len(result[\"imagePaths\"]) != 2:\n",
    "            continue\n",
    "\n",
    "        for img_path in result[\"imagePaths\"]:\n",
    "\n",
    "            if IN_DEBUG_MODE:\n",
    "                print(\"READING IMAGE AT: \" + img_path)\n",
    "\n",
    "            try:\n",
    "                i = Image.open(os.path.join(RESSOURCES_PATH, TRAIN_DATA_FOLDER, img_path))\n",
    "            except:\n",
    "                image_not_found = True\n",
    "                continue\n",
    "\n",
    "\n",
    "            # Scale down image (resize)\n",
    "            i = i.resize((IMAGE_WIDTH_PX, IMAGE_HEIGHT_PX))\n",
    "\n",
    "            # Channel order of Pillow is different than OpenCV\n",
    "            i = np.array(i)\n",
    "            # i = np.array(i)[:, :, ::-1]\n",
    "            i = hp.Preprocess.convert_to_BGR(i)\n",
    "\n",
    "            i = hp.Video.translate_image(i)\n",
    "\n",
    "            # Crop the image\n",
    "            i = i[0:115, 10:150]\n",
    "\n",
    "            i = hp.Preprocess.start(i)\n",
    "\n",
    "            img.append(i)\n",
    "\n",
    "        if image_not_found == False:\n",
    "            images.append(img)\n",
    "            try:\n",
    "                # np.array(images) # is only necessary to check if the data is homogenous\n",
    "                labels.append(result[\"positions\"])\n",
    "            except:\n",
    "                print(f\"Images shape got inhomogenous at: ${result['imagePaths']}\")\n",
    "                images.pop()\n",
    "\n",
    "            \n",
    "\n",
    "    if IN_DEBUG_MODE:\n",
    "        print(\"\\n\\n\")\n",
    "\n",
    "    return [np.array(images), np.array(labels)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base-Model generation\n",
    "LOSS_FUNCTION = 'categorical_crossentropy'\n",
    "\n",
    "input_branch_1 = keras.layers.Input(shape=(IMAGE_HEIGHT_PX - CROP_HEIGHT_PX, IMAGE_WIDTH_PX - CROP_WIDTH_PX, 3))\n",
    "input_branch_2 = keras.layers.Input(shape=(IMAGE_HEIGHT_PX - CROP_HEIGHT_PX, IMAGE_WIDTH_PX - CROP_WIDTH_PX, 3))\n",
    "\n",
    "# Shared convolutional layers for image processing\n",
    "convolutional_layers = [\n",
    "    keras.layers.Conv2D(32, (3, 3), activation='relu'),\n",
    "    keras.layers.MaxPooling2D((2, 2)),\n",
    "    keras.layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "    keras.layers.MaxPooling2D((2, 2)),\n",
    "    keras.layers.Conv2D(128, (3, 3), activation='relu'),\n",
    "    keras.layers.MaxPooling2D((2, 2)),\n",
    "    keras.layers.Flatten()\n",
    "]\n",
    "\n",
    "# Process first image\n",
    "x1 = input_branch_1\n",
    "for layer in convolutional_layers:\n",
    "    x1 = layer(x1)\n",
    "\n",
    "# Process second image\n",
    "x2 = input_branch_2\n",
    "for layer in convolutional_layers:\n",
    "    x2 = layer(x2)\n",
    "\n",
    "x = keras.layers.Concatenate(axis=-1)([x1, x2])\n",
    "x = keras.layers.Dense(256, activation='relu')(x)\n",
    "x = keras.layers.Dropout(0.2)(x)\n",
    "x = keras.layers.Dense(NUM_POSITIONS * NUM_CLASSES, activation='softmax')(x)  # Output layer with 8 * 4 units\n",
    "\n",
    "output = keras.layers.Reshape((NUM_POSITIONS, NUM_CLASSES))(x)\n",
    "\n",
    "# Build the model with the two input branches and the output layer\n",
    "base_model = keras.models.Model(inputs=[input_branch_1, input_branch_2], outputs=output)\n",
    "\n",
    "optimizer = keras.optimizers.legacy.Adam(learning_rate=0.001)\n",
    "\n",
    "base_model.compile(optimizer=optimizer, loss=LOSS_FUNCTION, metrics=['accuracy', 'mean_squared_error'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Base model with VGG16\n",
    "\n",
    "# Input layers for the two images\n",
    "input_branch_1 = keras.layers.Input(shape=(IMAGE_HEIGHT_PX - CROP_HEIGHT_PX, IMAGE_WIDTH_PX - CROP_WIDTH_PX, 3))\n",
    "input_branch_2 = keras.layers.Input(shape=(IMAGE_HEIGHT_PX - CROP_HEIGHT_PX, IMAGE_WIDTH_PX - CROP_WIDTH_PX, 3))\n",
    "\n",
    "# Load the pre-trained VGG16 model (without the top layer)\n",
    "base_model = VGG16(weights='imagenet', include_top=False, input_shape=(IMAGE_HEIGHT_PX - CROP_HEIGHT_PX, IMAGE_WIDTH_PX - CROP_WIDTH_PX, 3))\n",
    "\n",
    "# Freeze the pre-trained layers (optional)\n",
    "for layer in base_model.layers:\n",
    "  layer.trainable = False  # You can experiment with freezing some or all layers\n",
    "\n",
    "# Extract features from both images using the VGG16 base model\n",
    "x1 = base_model(input_branch_1)\n",
    "x2 = base_model(input_branch_2)\n",
    "\n",
    "# Concatenate the extracted features\n",
    "x = keras.layers.Concatenate(axis=-1)([x1, x2])\n",
    "\n",
    "# Add custom layers for your specific task\n",
    "x = keras.layers.Flatten()(x)\n",
    "x = keras.layers.Dense(256, activation='relu')(x)\n",
    "x = keras.layers.Dropout(0.2)(x)\n",
    "x = keras.layers.Dense(NUM_POSITIONS * NUM_CLASSES, activation='softmax')(x)  # Output layer with 8 * 4 units\n",
    "\n",
    "# Final output layer for multi-class classification (adjust based on your problem)\n",
    "output = keras.layers.Reshape((NUM_POSITIONS, NUM_CLASSES))(x)\n",
    "\n",
    "# Create the final model with two inputs and one output\n",
    "base_model = keras.models.Model(inputs=[input_branch_1, input_branch_2], outputs=output)\n",
    "\n",
    "# Compile the model (adjust optimizer and loss function as needed)\n",
    "base_model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-train existing model\n",
    "base_model = keras.models.load_model(os.path.join(MODEL_PATH, MODEL_NAME))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "22/22 [==============================] - 49s 2s/step - loss: 1.3797 - accuracy: 0.3066 - val_loss: 1.3569 - val_accuracy: 0.3338\n",
      "Epoch 2/10\n",
      "22/22 [==============================] - 52s 2s/step - loss: 1.3394 - accuracy: 0.3452 - val_loss: 1.3213 - val_accuracy: 0.3450\n",
      "Epoch 3/10\n",
      "22/22 [==============================] - 53s 2s/step - loss: 1.3058 - accuracy: 0.3727 - val_loss: 1.2972 - val_accuracy: 0.3837\n",
      "Epoch 4/10\n",
      "22/22 [==============================] - 53s 2s/step - loss: 1.2692 - accuracy: 0.4027 - val_loss: 1.2700 - val_accuracy: 0.3913\n",
      "Epoch 5/10\n",
      "22/22 [==============================] - 54s 2s/step - loss: 1.2569 - accuracy: 0.4075 - val_loss: 1.2565 - val_accuracy: 0.4112\n",
      "Epoch 6/10\n",
      "22/22 [==============================] - 54s 2s/step - loss: 1.2370 - accuracy: 0.4225 - val_loss: 1.2356 - val_accuracy: 0.4100\n",
      "Epoch 7/10\n",
      "22/22 [==============================] - 54s 2s/step - loss: 1.2109 - accuracy: 0.4507 - val_loss: 1.2221 - val_accuracy: 0.4269\n",
      "Epoch 8/10\n",
      "22/22 [==============================] - 55s 3s/step - loss: 1.1894 - accuracy: 0.4466 - val_loss: 1.2068 - val_accuracy: 0.4387\n",
      "Epoch 9/10\n",
      "22/22 [==============================] - 56s 3s/step - loss: 1.1824 - accuracy: 0.4595 - val_loss: 1.1838 - val_accuracy: 0.4450\n",
      "Epoch 10/10\n",
      "22/22 [==============================] - 59s 3s/step - loss: 1.1526 - accuracy: 0.4768 - val_loss: 1.1698 - val_accuracy: 0.4712\n"
     ]
    }
   ],
   "source": [
    "# Train model\n",
    "\n",
    "IN_DEBUG_MODE = False\n",
    "EPOCS = 10\n",
    "\n",
    "def fit_model(model, train_data, verify_data):\n",
    "    train_images = normalize_images(np.array(train_data[0]))\n",
    "    numberic_train_labels = np.array([map_labels_to_nummeric(label) for label in train_data[1]])\n",
    "    train_labels = keras.utils.to_categorical(numberic_train_labels, num_classes=NUM_CLASSES)\n",
    "\n",
    "    verify_images = normalize_images(np.array((verify_data[0])))\n",
    "    numberic_verify_labels = np.array([map_labels_to_nummeric(label) for label in verify_data[1]])\n",
    "    verify_labels = keras.utils.to_categorical(numberic_verify_labels, num_classes=NUM_CLASSES)\n",
    "\n",
    "    if IN_DEBUG_MODE: \n",
    "        print(\"----- SHAPES ------\\n\")\n",
    "        print(f\"Train labels shape: {train_labels.shape}\")\n",
    "        print(f\"Train images shape: {train_images.shape}\")\n",
    "\n",
    "        print(f\"Verify labels shape: {verify_labels.shape}\")\n",
    "        print(f\"Verify images shape: {verify_images.shape}\\n\\n\")\n",
    "\n",
    "    model.fit(\n",
    "        [train_images[:, 0], train_images[:, 1]], train_labels, \n",
    "        epochs=EPOCS, \n",
    "        validation_data=([verify_images[:, 0], verify_images[:, 1]], verify_labels), verbose=1)\n",
    "    \n",
    "    return model\n",
    "\n",
    "train_data = get_data(\"Train\")\n",
    "verify_data = get_data(\"Verify\")\n",
    "trained_model = fit_model(base_model, train_data, verify_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save trained model \n",
    "trained_model.save(os.path.join(MODEL_PATH, MODEL_NAME))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model\n",
    "model = keras.models.load_model(os.path.join(MODEL_PATH, MODEL_NAME))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 5s 1s/step\n",
      "\n",
      "\n",
      "\n",
      "------ PREDICTION: Index 6 --------\n",
      "\n",
      "NUMMERIC: \n",
      "\n",
      "[2 1 0 1 3 1 2 0]\n",
      "READABLE: \n",
      "\n",
      "['blue' 'yellow' 'red' 'yellow' '' 'yellow' 'blue' 'red']\n",
      "\n",
      "\n",
      "\n",
      "------ ACTUAL: Index 6 ------ \n",
      "\n",
      "NUMMERIC: \n",
      "\n",
      "[3 1 1 2 3 0 2 0]\n",
      "READABLE: \n",
      "\n",
      "['' 'yellow' 'yellow' 'blue' '' 'red' 'blue' 'red']\n"
     ]
    }
   ],
   "source": [
    "# Test model\n",
    "test_data = get_data(\"Test\")\n",
    "test_labels = np.array([map_labels_to_nummeric(label) for label in test_data[1]])\n",
    "\n",
    "if IN_DEBUG_MODE:\n",
    "    model.summary()\n",
    "\n",
    "test_images = normalize_images(np.array((test_data[0])))\n",
    "predictions = model.predict([test_images[:, 0], test_images[:, 1]])\n",
    "\n",
    "label_index = random.randint(0, len(test_labels)-1)\n",
    "\n",
    "print(\"\\n\\n\")\n",
    "print(f\"------ PREDICTION: Index {label_index + 1} --------\\n\")\n",
    "predicted_nummeric = np.argmax(predictions, axis=-1)\n",
    "predicted_readable = np.vectorize(label_mapping.get)(predicted_nummeric)\n",
    "actual_readable = np.vectorize(label_mapping.get)(test_labels)\n",
    "\n",
    "\n",
    "print(\"NUMMERIC: \\n\")\n",
    "print(predicted_nummeric[label_index])\n",
    "print(\"READABLE: \\n\")\n",
    "print(predicted_readable[label_index])\n",
    "print(\"\\n\\n\")\n",
    "\n",
    "print(f\"------ ACTUAL: Index {label_index + 1} ------ \\n\")\n",
    "print(\"NUMMERIC: \\n\")\n",
    "print(test_labels[label_index])\n",
    "print(\"READABLE: \\n\")\n",
    "print(actual_readable[label_index])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
