{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preconditions\n",
    "\n",
    "import os\n",
    "import json\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import keras.api._v2.keras as keras\n",
    "from keras.applications import VGG16, ResNet50\n",
    "from keras.callbacks import EarlyStopping, TensorBoard\n",
    "from keras.callbacks import ReduceLROnPlateau\n",
    "from keras import regularizers\n",
    "import tensorflow as tf\n",
    "import random\n",
    "\n",
    "import helper as hp\n",
    "\n",
    "os.environ[\"TF_NUM_INTEROP_THREADS\"] = \"1\"  # Number of inter-op threads\n",
    "os.environ[\"TF_NUM_INTRAOP_THREADS\"] = \"4\"  # Number of intra-op threads\n",
    "\n",
    "MODEL_NAME = \"v1_E50_PAT4_LR_001s_1000\" + \".keras\"\n",
    "MODEL_PATH = os.path.join(\"..\", \"tmp\", \"models\")\n",
    "\n",
    "RESSOURCES_PATH = os.path.join(\"..\", \"tmp\", \"train\", \"ressources\", \"bundle\")\n",
    "TRAIN_DATA_FOLDER = [\n",
    "    # \"1f7534fa-6ee2-4e6a-a921-2a635a5fe917\", \n",
    "    # \"9ef5620b-c769-49ba-b083-7cbc25fe7ec6\",\n",
    "    # \"56dd460c-200d-4248-b6d5-b61bd0681fd7\",\n",
    "    # \"81370b3a-0747-44f7-bdd9-08279027b99a\",\n",
    "    # \"470833f8-ddaf-4aca-8a65-e30f80504a8a\",\n",
    "    # \"32831052-7914-4220-bde4-c970c9c6c404\",\n",
    "    # \"82612320-658a-495f-9d10-e54b35471628\",\n",
    "    # \"a04111fd-ac8f-44b4-b1af-fe8f67252098\",\n",
    "    # \"b8768080-7bdd-43c1-ab11-b940b74b07ef\",\n",
    "    # \"efc4db14-5e6b-4c14-8543-997997d55476\"\n",
    "    # \"e80ee18d-5caf-420c-9c89-3062df2a30fb\",\n",
    "    # \"d164f634-b0af-46a6-a0f6-c85c0b462645\",\n",
    "    # \"ce40130e-f719-4921-ba4c-1529aa7158ba\",\n",
    "    # \"623bf116-6dcb-4293-860f-7c777f132c1a\",\n",
    "    # \"6dcf2542-014c-4e55-b8ff-2811c3f81fa2\"\n",
    "    # \"6b6f2d7c-5fcb-4912-a0f9-2f0ea405c766\",\n",
    "    # \"80b3b3b8-7521-415a-a584-24d2b94faa19\",\n",
    "    # \"2901923e-69a3-450a-8916-790fb6ab0ae6\",\n",
    "    # \"37470658-b16b-4878-9a55-29ffe7815a9a\",\n",
    "    # \"d67ed669-e26b-494c-b594-b01756e5a03f\"\n",
    "\n",
    "    # \"05c1d782-71b2-4e21-a857-97af015c2fc4\",\n",
    "    # \"3c9332f3-7f01-4881-8f34-10f7ad182ace\",\n",
    "    # \"10615d94-22fd-4dad-915b-bb3c74458200\",\n",
    "    # \"c52325af-e217-4b87-8e71-f24d9b5dbc44\",\n",
    "    # \"3d442115-ff43-4c76-9051-14df9ae62e37\"\n",
    "    # \"508ceb38-8d56-46cf-8537-c4c134758bff\"\n",
    "    # \"9e06cd91-0a34-4272-9afd-135424b077ce\"\n",
    "\n",
    "    # \"8db6808f-9497-4fb7-b541-a83c79c63c9d\",\n",
    "    # \"59db0623-7f36-48cf-a8b4-5cc63cc56746\",\n",
    "    # \"4ff3606d-d18e-42e5-876d-8518a663b868\",\n",
    "    # \"f84e6a0e-3497-4376-887b-6dbf6b443893\",\n",
    "    # \"c2b72ca1-f566-49ba-a298-53a5f6b37c7e\"\n",
    "\n",
    "    \"bfee6c3a-7fd7-42c5-b933-9cd8224623eb\"\n",
    "]\n",
    "\n",
    "# Image properties\n",
    "IMAGE_HEIGHT_PX = 120\n",
    "IMAGE_WIDTH_PX = 160\n",
    "CROP_HEIGHT_PX = 5\n",
    "CROP_WIDTH_PX = 20\n",
    "\n",
    "NUM_CHANNELS = 3\n",
    "NUM_CLASSES = 4\n",
    "NUM_POSITIONS = 8\n",
    "\n",
    "color_mapping = { 'red': 0, 'yellow': 1, 'blue': 2, '': 3 }\n",
    "label_mapping = { 0: 'red', 1: 'yellow', 2: 'blue', 3: ''}\n",
    "\n",
    "NORMALIZE_VALUE = 255\n",
    "\n",
    "def map_labels_to_nummeric(label):\n",
    "    mapped_label = []\n",
    "\n",
    "    for pos in label.values():\n",
    "        mapped_label.append(color_mapping[pos])\n",
    "\n",
    "    return mapped_label\n",
    "\n",
    "# Normalize the images so that all values are between 0 and 1\n",
    "def normalize_images(images):\n",
    "    return images / NORMALIZE_VALUE\n",
    "\n",
    "\n",
    "# Data loading\n",
    "IN_DEBUG_MODE = False\n",
    "IMAGE_FOLDER = \"Images\"\n",
    "LABELS_FOLDER = \"Labels\"\n",
    "JSON_NAME = \"scene_results.json\"\n",
    "\n",
    "def get_data(stage):\n",
    "    labels = []\n",
    "    images = []\n",
    "\n",
    "    for train_folder in TRAIN_DATA_FOLDER:\n",
    "\n",
    "        scene_results_path = os.path.join(RESSOURCES_PATH, train_folder, stage, JSON_NAME)\n",
    "\n",
    "        if IN_DEBUG_MODE:\n",
    "            print(\"CURRENT STAGE: \" + stage + \"\\n\")\n",
    "            print(\"READING SCENE RESULTS AT: \" + scene_results_path)\n",
    "\n",
    "        with open(scene_results_path, 'r') as file:\n",
    "            scene_results = json.load(file)\n",
    "\n",
    "        for result in scene_results:\n",
    "\n",
    "            img = [];\n",
    "\n",
    "            image_not_found = False\n",
    "\n",
    "            if len(result[\"imagePaths\"]) != 2:\n",
    "                continue\n",
    "\n",
    "            for img_path in result[\"imagePaths\"]:\n",
    "\n",
    "                if IN_DEBUG_MODE:\n",
    "                    print(\"READING IMAGE AT: \" + img_path)\n",
    "\n",
    "                try:\n",
    "                    i = Image.open(os.path.join(RESSOURCES_PATH, train_folder, img_path))\n",
    "                except:\n",
    "                    image_not_found = True\n",
    "                    continue\n",
    "\n",
    "\n",
    "                # Scale down image (resize)\n",
    "                i = i.resize((IMAGE_WIDTH_PX, IMAGE_HEIGHT_PX))\n",
    "\n",
    "                # Channel order of Pillow is different than OpenCV\n",
    "                i = np.array(i)\n",
    "                i = hp.Preprocess.convert_to_BGR(i)\n",
    "\n",
    "                # i = hp.Video.translate_image(i)\n",
    "\n",
    "                # Crop the image\n",
    "                i = i[0:115, 10:150]\n",
    "\n",
    "                # i = hp.Augmentation.black_spots(i, 10)\n",
    "\n",
    "                i = hp.Preprocess.start(i)\n",
    "\n",
    "                img.append(i)\n",
    "\n",
    "            if image_not_found == False:\n",
    "                images.append(img)\n",
    "                try:\n",
    "                    # np.array(images) # is only necessary to check if the data is homogenous\n",
    "                    labels.append(result[\"positions\"])\n",
    "                except:\n",
    "                    print(f\"Images shape got inhomogenous at: ${result['imagePaths']}\")\n",
    "                    images.pop()\n",
    "\n",
    "        if IN_DEBUG_MODE:\n",
    "            print(\"\\n\\n\")\n",
    "    \n",
    "\n",
    "\n",
    "    return [np.array(images), np.array(labels)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base-Model generation\n",
    "LOSS_FUNCTION = 'categorical_crossentropy'\n",
    "\n",
    "input_branch_1 = keras.layers.Input(shape=(IMAGE_HEIGHT_PX - CROP_HEIGHT_PX, IMAGE_WIDTH_PX - CROP_WIDTH_PX, 3))\n",
    "input_branch_2 = keras.layers.Input(shape=(IMAGE_HEIGHT_PX - CROP_HEIGHT_PX, IMAGE_WIDTH_PX - CROP_WIDTH_PX, 3))\n",
    "\n",
    "# Shared convolutional layers for image processing\n",
    "convolutional_layers = [\n",
    "    keras.layers.Conv2D(32, (3, 3), activation='relu'),\n",
    "    keras.layers.MaxPooling2D((2, 2)),\n",
    "    keras.layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "    keras.layers.MaxPooling2D((2, 2)),\n",
    "    keras.layers.Conv2D(128, (3, 3), activation='relu'),\n",
    "    keras.layers.MaxPooling2D((2, 2)),\n",
    "    keras.layers.Flatten()\n",
    "]\n",
    "\n",
    "# Process first image\n",
    "x1 = input_branch_1\n",
    "for layer in convolutional_layers:\n",
    "    x1 = layer(x1)\n",
    "\n",
    "# Process second image\n",
    "x2 = input_branch_2\n",
    "for layer in convolutional_layers:\n",
    "    x2 = layer(x2)\n",
    "\n",
    "x = keras.layers.Concatenate(axis=-1)([x1, x2])\n",
    "x = keras.layers.Dense(256, activation='relu')(x)\n",
    "x = keras.layers.Dropout(0.2)(x)\n",
    "x = keras.layers.Dense(NUM_POSITIONS * NUM_CLASSES, activation='softmax')(x)  # Output layer with 8 * 4 units\n",
    "\n",
    "output = keras.layers.Reshape((NUM_POSITIONS, NUM_CLASSES))(x)\n",
    "\n",
    "# Build the model with the two input branches and the output layer\n",
    "base_model = keras.models.Model(inputs=[input_branch_1, input_branch_2], outputs=output)\n",
    "\n",
    "optimizer = keras.optimizers.legacy.Adam(learning_rate=0.001)\n",
    "\n",
    "base_model.compile(optimizer=optimizer, loss=LOSS_FUNCTION, metrics=['accuracy', 'mean_squared_error'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base model generation v2\n",
    "LOSS_FUNCTION = 'categorical_crossentropy'\n",
    "\n",
    "base_model = keras.models.Sequential([\n",
    "    keras.layers.Conv2D(32, (3, 3), activation='relu', input_shape=(IMAGE_HEIGHT_PX - CROP_HEIGHT_PX, IMAGE_WIDTH_PX - CROP_WIDTH_PX, 3)),\n",
    "    keras.layers.Conv2D(32, (3, 3), activation='relu', input_shape=(IMAGE_HEIGHT_PX - CROP_HEIGHT_PX, IMAGE_WIDTH_PX - CROP_WIDTH_PX, 3), kernel_regularizer=keras.regularizers.l2(0.01)),\n",
    "    keras.layers.MaxPooling2D((2, 2)),\n",
    "    keras.layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "    keras.layers.Conv2D(64, (3, 3), activation='relu', kernel_regularizer=keras.regularizers.l2(0.01)),\n",
    "    keras.layers.MaxPooling2D((2, 2)),\n",
    "    keras.layers.Flatten(),\n",
    "\n",
    "    keras.layers.Dropout(0.2),\n",
    "    keras.layers.Dense(128, activation='relu', kernel_regularizer=keras.regularizers.l2(0.01)),\n",
    "    keras.layers.Dense(128, activation='relu')\n",
    "])\n",
    "\n",
    "input_image_1 = keras.layers.Input(shape=(IMAGE_HEIGHT_PX - CROP_HEIGHT_PX, IMAGE_WIDTH_PX - CROP_WIDTH_PX, 3), name='input_image_1')\n",
    "input_image_2 = keras.layers.Input(shape=(IMAGE_HEIGHT_PX - CROP_HEIGHT_PX, IMAGE_WIDTH_PX - CROP_WIDTH_PX, 3), name='input_image_2')\n",
    "\n",
    "encoded_left = base_model(input_image_1)\n",
    "encoded_right = base_model(input_image_2)\n",
    "\n",
    "concatenated = keras.layers.Concatenate(axis=-1)([encoded_left, encoded_right])\n",
    "\n",
    "dense_layer = keras.layers.Dense(NUM_POSITIONS * NUM_CLASSES, activation='softmax')(concatenated)\n",
    "reshaped_output = keras.layers.Reshape((NUM_POSITIONS, NUM_CLASSES))(dense_layer)\n",
    "\n",
    "base_model =  keras.models.Model(inputs=[input_image_1, input_image_2], outputs=reshaped_output)\n",
    "\n",
    "optimizer = keras.optimizers.legacy.Adam(learning_rate=0.001)\n",
    "\n",
    "base_model.compile(optimizer=optimizer, loss=LOSS_FUNCTION, metrics=['accuracy', 'mean_squared_error'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Base model with VGG16\n",
    "\n",
    "# Input layers for the two images\n",
    "input_branch_1 = keras.layers.Input(shape=(IMAGE_HEIGHT_PX - CROP_HEIGHT_PX, IMAGE_WIDTH_PX - CROP_WIDTH_PX, 3))\n",
    "input_branch_2 = keras.layers.Input(shape=(IMAGE_HEIGHT_PX - CROP_HEIGHT_PX, IMAGE_WIDTH_PX - CROP_WIDTH_PX, 3))\n",
    "\n",
    "# Load the pre-trained VGG16 model (without the top layer)\n",
    "base_model = VGG16(weights='imagenet', include_top=False, input_shape=(IMAGE_HEIGHT_PX - CROP_HEIGHT_PX, IMAGE_WIDTH_PX - CROP_WIDTH_PX, 3))\n",
    "\n",
    "# Freeze the pre-trained layers (optional)\n",
    "for layer in base_model.layers:\n",
    "  layer.trainable = False\n",
    "\n",
    "# Extract features from both images using the VGG16 base model\n",
    "x1 = base_model(input_branch_1)\n",
    "x2 = base_model(input_branch_2)\n",
    "\n",
    "# Concatenate the extracted features\n",
    "x = keras.layers.Concatenate(axis=-1)([x1, x2])\n",
    "\n",
    "# Add custom layers for your specific task\n",
    "x = keras.layers.Flatten()(x)\n",
    "x = keras.layers.Dense(256, activation='relu', kernel_regularizer=regularizers.l2(0.001))(x)\n",
    "# x = keras.layers.Dropout(0.3)(x)\n",
    "x = keras.layers.Dense(NUM_POSITIONS * NUM_CLASSES, activation='softmax')(x)  # Output layer with 8 * 4 units\n",
    "\n",
    "# Final output layer for multi-class classification (adjust based on your problem)\n",
    "output = keras.layers.Reshape((NUM_POSITIONS, NUM_CLASSES))(x)\n",
    "\n",
    "# Create the final model with two inputs and one output\n",
    "base_model = keras.models.Model(inputs=[input_branch_1, input_branch_2], outputs=output)\n",
    "\n",
    "# Compile the model (adjust optimizer and loss function as needed)\n",
    "base_model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy', 'mean_squared_error'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Base model with RESNET\n",
    "\n",
    "# Input layers for the two images\n",
    "input_branch_1 = keras.layers.Input(shape=(IMAGE_HEIGHT_PX - CROP_HEIGHT_PX, IMAGE_WIDTH_PX - CROP_WIDTH_PX, 3))\n",
    "input_branch_2 = keras.layers.Input(shape=(IMAGE_HEIGHT_PX - CROP_HEIGHT_PX, IMAGE_WIDTH_PX - CROP_WIDTH_PX, 3))\n",
    "\n",
    "# Load the pre-trained ResNET50 model (without the top layer)\n",
    "base_model = ResNet50(weights='imagenet', include_top=False, input_shape=(IMAGE_HEIGHT_PX - CROP_HEIGHT_PX, IMAGE_WIDTH_PX - CROP_WIDTH_PX, 3))\n",
    "\n",
    "# Freeze the pre-trained layers (optional)\n",
    "for layer in base_model.layers:\n",
    "  layer.trainable = False\n",
    "\n",
    "# Extract features from both images using the ResNet50 base model\n",
    "x1 = base_model(input_branch_1)\n",
    "x2 = base_model(input_branch_2)\n",
    "\n",
    "# Concatenate the extracted features\n",
    "x = keras.layers.Concatenate(axis=-1)([x1, x2])\n",
    "\n",
    "# Add custom layers for your specific task\n",
    "x = keras.layers.Flatten()(x)\n",
    "x = keras.layers.Dense(256, activation='relu', kernel_regularizer=regularizers.l2(0.001))(x)\n",
    "# x = keras.layers.Dropout(0.3)(x)\n",
    "x = keras.layers.Dense(NUM_POSITIONS * NUM_CLASSES, activation='softmax')(x)  # Output layer with 8 * 4 units\n",
    "\n",
    "# Final output layer for multi-class classification (adjust based on your problem)\n",
    "output = keras.layers.Reshape((NUM_POSITIONS, NUM_CLASSES))(x)\n",
    "\n",
    "# Create the final model with two inputs and one output\n",
    "base_model = keras.models.Model(inputs=[input_branch_1, input_branch_2], outputs=output)\n",
    "\n",
    "# Compile the model (adjust optimizer and loss function as needed)\n",
    "base_model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy', 'mean_squared_error'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-train existing model\n",
    "base_model = keras.models.load_model(os.path.join(MODEL_PATH, \"v5_no_base_70000.keras\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = get_data(\"Train\")\n",
    "verify_data = get_data(\"Verify\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "22/22 [==============================] - 20s 906ms/step - loss: 2.6122 - accuracy: 0.3923 - mean_squared_error: 0.2332 - val_loss: 1.8739 - val_accuracy: 0.4200 - val_mean_squared_error: 0.2318 - lr: 0.0010\n",
      "Epoch 2/50\n",
      "22/22 [==============================] - 20s 912ms/step - loss: 1.6332 - accuracy: 0.4448 - mean_squared_error: 0.2300 - val_loss: 1.5157 - val_accuracy: 0.4406 - val_mean_squared_error: 0.2297 - lr: 0.0010\n",
      "Epoch 3/50\n",
      "22/22 [==============================] - 20s 925ms/step - loss: 1.3867 - accuracy: 0.4950 - mean_squared_error: 0.2271 - val_loss: 1.3582 - val_accuracy: 0.4988 - val_mean_squared_error: 0.2280 - lr: 0.0010\n",
      "Epoch 4/50\n",
      "22/22 [==============================] - 20s 932ms/step - loss: 1.2340 - accuracy: 0.5539 - mean_squared_error: 0.2236 - val_loss: 1.2331 - val_accuracy: 0.5481 - val_mean_squared_error: 0.2229 - lr: 0.0010\n",
      "Epoch 5/50\n",
      "22/22 [==============================] - 21s 965ms/step - loss: 1.1022 - accuracy: 0.6391 - mean_squared_error: 0.2181 - val_loss: 1.1996 - val_accuracy: 0.6069 - val_mean_squared_error: 0.2206 - lr: 0.0010\n",
      "Epoch 6/50\n",
      "22/22 [==============================] - 21s 966ms/step - loss: 0.9877 - accuracy: 0.7088 - mean_squared_error: 0.2137 - val_loss: 1.1369 - val_accuracy: 0.6463 - val_mean_squared_error: 0.2166 - lr: 0.0010\n",
      "Epoch 7/50\n",
      "22/22 [==============================] - 21s 965ms/step - loss: 0.8604 - accuracy: 0.7709 - mean_squared_error: 0.2103 - val_loss: 1.0822 - val_accuracy: 0.6956 - val_mean_squared_error: 0.2137 - lr: 0.0010\n",
      "Epoch 8/50\n",
      "22/22 [==============================] - 21s 973ms/step - loss: 0.7793 - accuracy: 0.8166 - mean_squared_error: 0.2081 - val_loss: 1.0778 - val_accuracy: 0.7069 - val_mean_squared_error: 0.2133 - lr: 0.0010\n",
      "Epoch 9/50\n",
      "22/22 [==============================] - 22s 991ms/step - loss: 0.6537 - accuracy: 0.8637 - mean_squared_error: 0.2077 - val_loss: 0.9870 - val_accuracy: 0.7500 - val_mean_squared_error: 0.2129 - lr: 0.0010\n",
      "Epoch 10/50\n",
      "22/22 [==============================] - 22s 992ms/step - loss: 0.5871 - accuracy: 0.8820 - mean_squared_error: 0.2079 - val_loss: 0.9660 - val_accuracy: 0.7494 - val_mean_squared_error: 0.2125 - lr: 0.0010\n",
      "Epoch 11/50\n",
      "22/22 [==============================] - 22s 982ms/step - loss: 0.5367 - accuracy: 0.9062 - mean_squared_error: 0.2070 - val_loss: 0.9730 - val_accuracy: 0.7638 - val_mean_squared_error: 0.2121 - lr: 0.0010\n",
      "Epoch 12/50\n",
      "22/22 [==============================] - 22s 980ms/step - loss: 0.5255 - accuracy: 0.9073 - mean_squared_error: 0.2069 - val_loss: 0.8719 - val_accuracy: 0.7862 - val_mean_squared_error: 0.2105 - lr: 0.0010\n",
      "Epoch 13/50\n",
      "22/22 [==============================] - 22s 983ms/step - loss: 0.4368 - accuracy: 0.9418 - mean_squared_error: 0.2070 - val_loss: 0.8118 - val_accuracy: 0.8206 - val_mean_squared_error: 0.2121 - lr: 0.0010\n",
      "Epoch 14/50\n",
      "22/22 [==============================] - 22s 986ms/step - loss: 0.4000 - accuracy: 0.9454 - mean_squared_error: 0.2081 - val_loss: 0.9430 - val_accuracy: 0.7856 - val_mean_squared_error: 0.2136 - lr: 0.0010\n",
      "Epoch 15/50\n",
      "22/22 [==============================] - 22s 987ms/step - loss: 0.4142 - accuracy: 0.9425 - mean_squared_error: 0.2072 - val_loss: 0.8723 - val_accuracy: 0.8150 - val_mean_squared_error: 0.2125 - lr: 0.0010\n",
      "Epoch 16/50\n",
      "22/22 [==============================] - 22s 987ms/step - loss: 0.3487 - accuracy: 0.9712 - mean_squared_error: 0.2076 - val_loss: 0.8018 - val_accuracy: 0.8087 - val_mean_squared_error: 0.2128 - lr: 5.0000e-04\n",
      "Epoch 17/50\n",
      "22/22 [==============================] - 22s 988ms/step - loss: 0.2676 - accuracy: 0.9891 - mean_squared_error: 0.2081 - val_loss: 0.7490 - val_accuracy: 0.8375 - val_mean_squared_error: 0.2129 - lr: 5.0000e-04\n",
      "Epoch 18/50\n",
      "22/22 [==============================] - 22s 988ms/step - loss: 0.2313 - accuracy: 0.9929 - mean_squared_error: 0.2082 - val_loss: 0.7362 - val_accuracy: 0.8406 - val_mean_squared_error: 0.2128 - lr: 5.0000e-04\n",
      "Epoch 19/50\n",
      "22/22 [==============================] - 22s 988ms/step - loss: 0.2110 - accuracy: 0.9930 - mean_squared_error: 0.2086 - val_loss: 0.7020 - val_accuracy: 0.8388 - val_mean_squared_error: 0.2131 - lr: 5.0000e-04\n",
      "Epoch 20/50\n",
      "22/22 [==============================] - 22s 994ms/step - loss: 0.2040 - accuracy: 0.9909 - mean_squared_error: 0.2089 - val_loss: 0.6665 - val_accuracy: 0.8481 - val_mean_squared_error: 0.2133 - lr: 5.0000e-04\n",
      "Epoch 21/50\n",
      "22/22 [==============================] - 22s 989ms/step - loss: 0.1988 - accuracy: 0.9896 - mean_squared_error: 0.2092 - val_loss: 0.7116 - val_accuracy: 0.8356 - val_mean_squared_error: 0.2127 - lr: 5.0000e-04\n",
      "Epoch 22/50\n",
      "22/22 [==============================] - 22s 990ms/step - loss: 0.1865 - accuracy: 0.9907 - mean_squared_error: 0.2093 - val_loss: 0.6727 - val_accuracy: 0.8550 - val_mean_squared_error: 0.2136 - lr: 5.0000e-04\n",
      "Epoch 23/50\n",
      "22/22 [==============================] - 22s 994ms/step - loss: 0.1689 - accuracy: 0.9946 - mean_squared_error: 0.2093 - val_loss: 0.6679 - val_accuracy: 0.8469 - val_mean_squared_error: 0.2133 - lr: 2.5000e-04\n",
      "Epoch 24/50\n",
      "22/22 [==============================] - 22s 992ms/step - loss: 0.1564 - accuracy: 0.9975 - mean_squared_error: 0.2093 - val_loss: 0.6589 - val_accuracy: 0.8438 - val_mean_squared_error: 0.2141 - lr: 2.5000e-04\n"
     ]
    }
   ],
   "source": [
    "# Train model\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=4, min_delta=0.01, restore_best_weights=True)\n",
    "learning_rate_scheduler = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=2, min_delta=0.01)\n",
    "tensorboard_callback = TensorBoard(log_dir='./logs')\n",
    "\n",
    "IN_DEBUG_MODE = False\n",
    "EPOCS = 50\n",
    "\n",
    "def fit_model(model, train_data, verify_data):\n",
    "    train_images = normalize_images(np.array(train_data[0]))\n",
    "    numberic_train_labels = np.array([map_labels_to_nummeric(label) for label in train_data[1]])\n",
    "    train_labels = keras.utils.to_categorical(numberic_train_labels, num_classes=NUM_CLASSES)\n",
    "\n",
    "    verify_images = normalize_images(np.array((verify_data[0])))\n",
    "    numberic_verify_labels = np.array([map_labels_to_nummeric(label) for label in verify_data[1]])\n",
    "    verify_labels = keras.utils.to_categorical(numberic_verify_labels, num_classes=NUM_CLASSES)\n",
    "\n",
    "    if IN_DEBUG_MODE: \n",
    "        print(\"----- SHAPES ------\\n\")\n",
    "        print(f\"Train labels shape: {train_labels.shape}\")\n",
    "        print(f\"Train images shape: {train_images.shape}\")\n",
    "\n",
    "        print(f\"Verify labels shape: {verify_labels.shape}\")\n",
    "        print(f\"Verify images shape: {verify_images.shape}\\n\\n\")\n",
    "\n",
    "    model.fit(\n",
    "        [train_images[:, 0], train_images[:, 1]], train_labels, \n",
    "        epochs=EPOCS, \n",
    "        validation_data=([verify_images[:, 0], verify_images[:, 1]], verify_labels), verbose=1,\n",
    "        callbacks=[early_stopping, learning_rate_scheduler, tensorboard_callback])\n",
    "    \n",
    "    return model\n",
    "\n",
    "trained_model = fit_model(base_model, train_data, verify_data)\n",
    "trained_model.save(os.path.join(MODEL_PATH, MODEL_NAME))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save trained model \n",
    "trained_model.save(os.path.join(MODEL_PATH, MODEL_NAME))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model\n",
    "model = keras.models.load_model(os.path.join(MODEL_PATH, MODEL_NAME))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = get_data(\"Test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 2s 74ms/step\n",
      "\n",
      "\n",
      "\n",
      "------ PREDICTION: Index 394 --------\n",
      "\n",
      "NUMMERIC: \n",
      "\n",
      "[3 1 3 2 3 2 3 0]\n",
      "READABLE: \n",
      "\n",
      "['' 'yellow' '' 'blue' '' 'blue' '' 'red']\n",
      "\n",
      "\n",
      "\n",
      "------ ACTUAL: Index 394 ------ \n",
      "\n",
      "NUMMERIC: \n",
      "\n",
      "[3 1 1 2 3 2 2 0]\n",
      "READABLE: \n",
      "\n",
      "['' 'yellow' 'yellow' 'blue' '' 'blue' 'blue' 'red']\n"
     ]
    }
   ],
   "source": [
    "# Test model\n",
    "test_labels = np.array([map_labels_to_nummeric(label) for label in test_data[1]])\n",
    "\n",
    "if IN_DEBUG_MODE:\n",
    "    model.summary()\n",
    "\n",
    "test_images = normalize_images(np.array((test_data[0])))\n",
    "predictions = model.predict([test_images[:, 0], test_images[:, 1]])\n",
    "\n",
    "label_index = random.randint(0, len(test_labels)-1)\n",
    "\n",
    "print(\"\\n\\n\")\n",
    "print(f\"------ PREDICTION: Index {label_index + 1} --------\\n\")\n",
    "predicted_nummeric = np.argmax(predictions, axis=-1)\n",
    "predicted_readable = np.vectorize(label_mapping.get)(predicted_nummeric)\n",
    "actual_readable = np.vectorize(label_mapping.get)(test_labels)\n",
    "\n",
    "\n",
    "print(\"NUMMERIC: \\n\")\n",
    "print(predicted_nummeric[label_index])\n",
    "print(\"READABLE: \\n\")\n",
    "print(predicted_readable[label_index])\n",
    "print(\"\\n\\n\")\n",
    "\n",
    "print(f\"------ ACTUAL: Index {label_index + 1} ------ \\n\")\n",
    "print(\"NUMMERIC: \\n\")\n",
    "print(test_labels[label_index])\n",
    "print(\"READABLE: \\n\")\n",
    "print(actual_readable[label_index])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
